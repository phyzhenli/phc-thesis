\begin{abstract}
    人工智能的快速发展需要海量的算力支持，导致计算需求急剧增加，消耗了大量能源。同时，集成电路在可穿戴设备、便携设备和数据中心等场景中的功耗问题日益严峻，研究者需要寻找新的方法以降低系统功耗，提高芯片能效。
    近似计算作为一种新兴的计算范式，允许系统在可接受的误差范围内返回结果。与容错应用结合，它能够在满足精度需求的前提下提高计算效率，降低芯片能耗。
    % 因此，在数字信号处理、机器学习等场景中，近似计算得到了工业界和学术界的广泛关注。
    
    近似电路设计是近似计算的一个分支，旨在通过对电路中的精确算术单元引入近似，以降低硬件开销。乘法是一种常见的计算操作，在许多应用中被频繁调用。为了提高计算效率，研究人员提出了近似乘法器，对乘法操作在硬件上进行优化。然而，现有的近似乘法器相关工作一方面没有同时考虑数据分布和输入极性，另一方面基于低效的手工设计方法。因此本文提出了两种自动化近似乘法器设计方法，能够高效地生成不同精度的适用于专用集成电路（Application-Specific Integrated Circuit, ASIC）和现场可编程门阵列（Field Programmable Gate Array, FPGA）的高质量近似乘法器。基于得到的近似乘法器，本文利用强化学习方法对深度神经网络（Deep Neural Network, DNN）硬件加速器进行了近似逻辑综合研究。具体内容如下：
    
    （1）面向 ASIC，本文基于数据分布和输入极性提出了一个自动化近似乘法器设计方法，该方法能够高效地生成适应于特定应用的高性能ASIC近似乘法器，提高应用的运算效率。
    具体来说，本文提出的方法在对乘法器的部分积累加求和前，引入与、或、异或和移位操作来压缩部分积，降低部分积阵列的规模，减轻累加压力。本文利用改进的Baugh-Wooley算法支持了补码有符号乘法器。为了能够利用计算机自动化求解，本文将寻找较优压缩操作的问题建模成数学问题，该问题的目标函数同时考虑了乘法器的精度和硬件开销，之后利用混合整数遗传算法进行搜索。
    本文对所提出的方法进行了实验，实验结果表明基于均匀分布得到的8比特无符号乘法器领先于国际前沿工作。针对基于8比特无符号数量化的三个不同规模的DNN生成的乘法器，在精度损失不超过0.01\%的情况下实现了26.4\%-47.6\%的硬件性能收益。此外，面向自适应滤波器生成的16位补码有符号乘法器在峰值信噪比（Peak Signal-to-Noise Ratio, PSNR）损失较小的情况下实现了27.1\%的硬件成本提升。最后，基于32比特半正态分布的实验结果表明提出的方法对大位宽乘法器同样有效。
    
    （2）由于ASIC和FPGA底层架构不同，ASIC近似乘法器通常无法在FPGA上取得相同比例的硬件性能提升，因此面向FPGA应用，本文基于贝叶斯优化提出了一个自动化近似乘法器生成方法，避免了手工修改查找表编码方法效率较低的问题。
    该方法假设乘法器的部分积在生成后、累加前存在一次由半加器阵列进行的压缩操作（与（1）中的压缩操作不同，这里是半加操作）。本文利用贝叶斯优化，基于所提出的4种半加器简化方法对半加器阵列进行优化。优化后，该方法保留后续累加过程中部分积的粗粒度加法，使其能够被电子设计自动化（Electronic Design Automation, EDA）工具高效地识别并映射到FPGA的快速进位链。与国际前沿工作中的1167个近似乘法器相比，本文生成的乘法器位于帕累拖前沿，精度和硬件开销综合指标平均提高了28.70\%-38.47\%。
    
    （3）结合前面两个工作，面向大规模电路，本文基于强化学习方法进行了近似逻辑综合研究。具体来讲，本文首先提出了一个基于最大无扇出锥（Maximum Fanout-Free Cone, MFFC）自适应超图划分的端到端强化学习逻辑优化框架，对大规模电路进行全面优化，以改善芯片的面积、延迟和功耗。
    % 该框架首先利用硬件描述语言解析工具Yosys对电路进行读入和解析，接着提取电路中组合逻辑，利用“自然划分”和MFFC超图划分将提取的组合逻辑分割成多个子电路，并利用强化学习序列优化方法对所有的子电路并行探索，最后由商业综合工具评估面积和延迟结果。
    本文对超过150个电路进行实验，实验结果表明所提出的方法与ABC resyn2 相比，面积延迟积平均提高了5.17\%。之后，本文将提出的强化学习逻辑优化框架与基于数据分布和输入极性得到的DNN近似乘法器库结合，对不同近似乘法器实现的DNN硬件加速器进行了实验，实验结果显示近似乘法器的单独硬件开销与对应加速器的硬件开销的改善比例存在一定偏差。然而，基于帕累拖前沿的乘法器实现的加速器仍处于帕累拖前沿。

    以上三个研究工作表明，面向大规模电路，硬件设计师在使用本文或其他的方法得到近似乘法器库后，应对得到的帕累拖前沿乘法器进行探索以确定最佳的硬件实现。
    
\end{abstract}
    
\begin{abstract*}
      With the continuous development of artificial intelligence, there has been a sharp increase in computational demands, requiring massive computing power and resulting in significant energy consumption. At the same time, power consumption has become a pressing issue for integrated circuits in scenarios such as wearable devices, portable devices, and data centers. People are seeking new methods to reduce system power consumption and improve chip efficiency.
      Approximate computing is an emerging computing paradigm that allows systems to return results within an acceptable range of error. When combined with fault-tolerant applications, it can improve computational efficiency and reduce chip power consumption while meeting accuracy requirements. As a result, approximate computing has gained widespread attention in the industry and academia, especially in fields like digital signal processing and machine learning.
    
      Approximate circuit design is a branch of approximate computing that involves introducing approximation into the arithmetic units of a circuit to reduce hardware costs. Multiplication is a resource-intensive operation in many applications. To improve efficiency, researchers have proposed approximate multipliers to optimize the hardware implementation of frequently invoked multiplication operations in various applications. However, existing methods for designing approximate multipliers either lack simultaneous consideration of data distribution and input polarity or rely on time-consuming manual design approaches. Therefore, this paper presents two automated methods capable of generating high-quality approximate multipliers with different precisions for Application-Specific Integrated Circuits (ASICs) and Field Programmable Gate Arrays (FPGAs) in a short time. Additionally, by integrating the generated approximate multiplier library with logic synthesis, this paper investigates the hardware acceleration of Deep Neural Networks (DNNs) based on different approximate multipliers using approximate logic synthesis. The specific details are as follows:
    
      (1) This thesis proposes an open-source high-quality automated approximate multiplier generation method for application-specific integrated circuits (ASICs) that considers data distribution and input polarity. This method can generate a large number of high-performance ASIC approximate multipliers suitable for specific applications in a short time, thereby improving computational efficiency. The method uses AND, OR, XOR, and shift operations to compress partial products before accumulation, reducing the size of the partial product array and alleviating the accumulation pressure. Based on the improved Baugh-Wooley algorithm, the method can generate two's complement signed multipliers. To automatically search the optimal compression operations, the problem of finding the best compression operations is defined as a mathematical problem, then a mixed integer genetic algorithm is used to solve the problem. Extensive experiments show that the generated 8-bit unsigned multipliers under uniform distribution outperform the state-of-the-art works. For the multipliers generated for LeNet, AlexNet, and VGG16 using 8-bit unsigned quantization, the performance gains range from 26.4\% to 47.6\% while maintaining accuracy loss below 0.01\%. The 16-bit two's complement signed multiplier generated for adaptive filters achieves a 27.1\% hardware improvement with negligible peak signal-to-noise ratio (PSNR) loss. The experimental results based on a 32-bit half-normal distribution demonstrate the effectiveness of the method for large-bit-width multipliers.
    
      (2) This thesis proposes an open-source automated approximate multiplier generation method based on Bayesian optimization for field programmable gate array (FPGA), thereby avoiding the laborious and time-consuming process of manually modifying lookup table encoding methods. The method assumes that there is a compression process performed by a half-adder array on the partial products after generation and before accumulation.
      The half-adder array is optimized using the Bayesian optimization algorithm based on four proposed half-adder simplification methods. Then the method preserves the coarse-grained additions in the subsequent accumulation, which can be easily mapped to the FPGA's fast carry chains by electronic design automation (EDA) tools.
      Compared to 1167 state-of-the-art approximate multipliers, the generated multipliers form a Pareto front with an average improvement of 28.70\% to 38.47\%.
    
      (3) This thesis proposes an open-source end-to-end reinforcement learning logic optimization framework based on adaptive maximum fanout-free cone (MFFC) hypergraph partitioning, capable of thoroughly optimizing large-scale circuits to enhance chip area, latency, and power consumption.. The framework uses Yosys to parse verilog and extract the combinational logic from the circuit, which then be divided into multiple sub-circuits using "natural partitioning" and MFFC hypergraph partitioning. It explores all sub-circuits in parallel using the proposed reinforcement learning sequence optimization method and evaluates the results using the commercial synthesis tool. Based on more than 150 benchmarks, experimental results show that the proposed method achieves an average improvement of 5.17\% in area-delay product compared to ABC resyn2. 
      
      (4) Combining the logic optimization framework with the generated approximate multiplier libraries, this thesis explore many deep neural network (DNN) hardware accelerators based on different approximate multipliers. The results indicate that the increase in hardware cost for individual approximate multipliers deviates from the corresponding accelerator's hardware cost increase. However, the hardware overhead of accelerators corresponding to multipliers at the Pareto front still remains at the Pareto front. Thus exploring Pareto-front multipliers in the library can achieve the optimal hardware implementation.
\end{abstract*}