\begin{abstract}
    随着人工智能的不断发展，计算需求急剧增加，需要海量的算力进行支撑，带来大量的能源消耗。同时，在可穿戴设备、便携设备和数据中心等场景，集成电路面临的功耗问题日益严峻，人们需要寻找新的方法降低系统功耗，提高芯片能效。
    近似计算是一种新兴的计算范式，允许系统在可接受的误差范围内返回结果，与容错应用结合，在满足精度需求的前提下能够提高计算效率，降低芯片能耗。因此，在数字信号处理、机器学习等场景中，近似计算得到了工业界和学术界的广泛关注。
    
    近似电路设计是近似计算的一个分支，是指通过对电路中的精确算术单元引入近似，达到降低硬件开销的目的。
    针对近似乘法器，本文进行了多方面的研究，包括：
    
    （1）提出并开源了一个考虑数据分布和输入极性的面向专用集成电路（Application-Specific Integrated Circuit, ASIC）的高质量自动化近似乘法器生成方法，该方法在对部分积进行累加求和之前，引入与、或、异或和移位操作对部分积进行压缩，降低部分积阵列的规模，减轻后续的累加压力。基于改进的Baugh-Wooley算法，方法经过扩展后实现了对补码有符号乘法器的支持。为了能够自动化求解，将寻找较优压缩操作的问题建模成数学问题，利用混合整数遗传算法进行搜索。
    本文对所提出的方法进行了大量实验，结果表明均匀分布下生成的8比特无符号乘法器大幅优于国际前沿工作，针对采用8比特无符号数量化的LeNet、AlexNet和VGG16生成的乘法器在精度损失不超过0.01\%的情况下实现了26.4\%-47.6\%的性能收益，面向自适应滤波器生成的16位补码有符号乘法器在峰值信噪比（Peak Signal-to-Noise Ratio, PSNR）损失可以忽略的情况下实现了27.1\%的提升，基于32比特半正态分布的实验结果表明提出的方法对大位宽乘法器同样有效。
    
    （2）提出并开源了一个面向现场可编程门阵列（Field Programmable Gate Array, FPGA）应用的基于贝叶斯优化的自动化近似乘法器生成方法，该方法假设乘法器的部分积在生成后、累加前存在一次由半加器阵列进行的压缩操作，利用贝叶斯优化基于提出的4种半加器简化方法对半加器阵列进行优化，保留后续累加过程中部分积的粗粒度加法，使其能够被电子设计自动化（Electronic Design Automation, EDA）工具轻易地识别并映射到FPGA的快速进位链。与国际前沿工作中的1167个近似乘法器相比，生成的乘法器能够形成帕累拖前沿，综合指标平均提高了28.70\%-38.47\%。
    
    （3）提出并开源了一个基于最大无扇出锥（Maximum Fanout-Free Cone, MFFC）自适应超图划分的端到端强化学习逻辑优化框架，该框架首先利用Yosys对电路进行读入和解析，接着将电路中的组合逻辑提取出来，利用“自然划分”和MFFC超图划分将提取的组合逻辑分割成多个子电路，对所有的子电路利用提出的强化学习序列优化方法并行地进行探索，最后由商业综合工具评估结果。基于超过150个电路的实验结果表明，提出的方法与ABC resyn2 相比，面积延迟积平均提高了5.17\%。将框架与近似乘法器库结合，对基于不同近似乘法器实现的深度神经网络（Deep Neural Network, DNN）硬件加速器进行了探究，结果显示近似乘法器的单独硬件成本提升与对应加速器的硬件成本提升存在一定偏差，但处于帕累拖前沿的乘法器对应的加速器的硬件开销仍处于帕累拖前沿，在实际使用中可对库中的帕累拖前沿乘法器进行探索以确定最佳硬件实现。
    
\end{abstract}
    
\begin{abstract*}
      With the continuous development of artificial intelligence, there has been a sharp increase in computational demands, requiring massive computing power and resulting in significant energy consumption. At the same time, power consumption has become a pressing issue for integrated circuits in scenarios such as wearable devices, portable devices, and data centers. People are seeking new methods to reduce system power consumption and improve chip efficiency.
      Approximate computing is an emerging computing paradigm that allows systems to return results within an acceptable range of error. When combined with fault-tolerant applications, it can improve computational efficiency and reduce chip power consumption while meeting accuracy requirements. As a result, approximate computing has gained widespread attention in the industry and academia, especially in fields like digital signal processing and machine learning.
    
      Approximate circuit design is a branch of approximate computing that aims to reduce hardware costs by introducing approximation into precise arithmetic units. This thesis focuses on the approximate multiplier design and includes the following aspects:
    
      (1) This thesis proposes an open-source high-quality automated approximate multiplier generation method for application-specific integrated circuits (ASICs) that considers data distribution and input polarity. The method uses AND, OR, XOR, and shift operations to compress partial products before accumulation, reducing the size of the partial product array and alleviating the accumulation pressure. Based on the improved Baugh-Wooley algorithm, the method can generate two's complement signed multipliers. To automatically search the optimal compression operations, the problem of finding the best compression operations is defined as a mathematical problem, then a mixed integer genetic algorithm is used to solve the problem. Extensive experiments show that the generated 8-bit unsigned multipliers under uniform distribution outperform the state-of-the-art works. For the multipliers generated for LeNet, AlexNet, and VGG16 using 8-bit unsigned quantization, the performance gains range from 26.4\% to 47.6\% while maintaining accuracy loss below 0.01\%. The 16-bit two's complement signed multiplier generated for adaptive filters achieves a 27.1\% hardware improvement with negligible peak signal-to-noise ratio (PSNR) loss. The experimental results based on a 32-bit half-normal distribution demonstrate the effectiveness of the method for large-bit-width multipliers.
    
      (2) This thesis proposes an open-source automated approximate multiplier generation method based on Bayesian optimization for field programmable gate array (FPGA). The method assumes that there is a compression process performed by a half-adder array on the partial products after generation and before accumulation.
      The half-adder array is optimized using the Bayesian optimization algorithm based on four proposed half-adder simplification methods. Then the method preserves the coarse-grained additions in the subsequent accumulation, which can be easily mapped to the FPGA's fast carry chains by electronic design automation (EDA) tools.
      Compared to 1167 state-of-the-art approximate multipliers, the generated multipliers form a Pareto front with an average improvement of 28.70\% to 38.47\%.
    
      (3) This thesis proposes an open-source end-to-end reinforcement learning logic optimization framework based on adaptive maximum fanout-free cone (MFFC) hypergraph partitioning. The framework uses Yosys to parse verilog and extract the combinational logic from the circuit, which then be divided into multiple sub-circuits using "natural partitioning" and MFFC hypergraph partitioning. It explores all sub-circuits in parallel using the proposed reinforcement learning sequence optimization method and evaluates the results using the commercial synthesis tool. Based on more than 150 benchmarks, experimental results show that the proposed method achieves an average improvement of 5.17\% in area-delay product compared to ABC resyn2. 
      Combining the framework with approximate multiplier libraries, exploration was conducted on deep neural network (DNN) hardware accelerators based on different approximate multipliers. The results indicate that the increase in hardware cost for individual approximate multipliers deviates from the corresponding accelerator's hardware cost increase. However, the hardware overhead of accelerators corresponding to multipliers at the Pareto front still remains at the Pareto front. Thus exploring Pareto-front multipliers in the library can achieve the optimal hardware implementation.
\end{abstract*}