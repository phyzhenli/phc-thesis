\begin{abstract}
    人工智能的快速发展需要海量的算力支持，导致计算需求急剧增加，消耗了大量能源。同时，集成电路在可穿戴设备、便携设备和数据中心等场景中的功耗问题日益严峻，研究者需要寻找新的方法以降低系统功耗，提高芯片能效。
    近似计算作为一种新兴的计算范式，允许系统在可接受的误差范围内返回结果。与容错应用结合，它能够在满足精度需求的前提下提高计算效率，降低芯片能耗。
    % 因此，在数字信号处理、机器学习等场景中，近似计算得到了工业界和学术界的广泛关注。
    
    近似电路设计是近似计算的一个分支，旨在通过对电路中的精确算术单元引入近似，以降低硬件开销。乘法是一种常见的计算操作，在许多应用中被频繁调用。为了提高计算效率，研究人员提出了近似乘法器，对乘法操作在硬件上进行优化。然而，现有的近似乘法器相关工作一方面没有同时考虑数据分布和输入极性，另一方面基于低效的手工设计方法。因此本文提出了两种自动化近似乘法器设计方法，能够高效地生成不同精度的适用于专用集成电路（Application-Specific Integrated Circuit, ASIC）和现场可编程门阵列（Field Programmable Gate Array, FPGA）的高质量近似乘法器。基于得到的近似乘法器，本文利用强化学习方法对深度神经网络（Deep Neural Network, DNN）硬件加速器进行了近似逻辑综合研究。具体内容如下：
    
    （1）面向 ASIC，本文基于数据分布和输入极性提出了一个自动化近似乘法器设计方法，该方法能够高效地生成适应于特定应用的高性能ASIC近似乘法器，提高应用的运算效率。
    具体来说，本文提出的方法在对乘法器的部分积累加求和前，引入与、或、异或和移位操作来压缩部分积，降低部分积阵列的规模，减轻累加压力。本文利用改进的Baugh-Wooley算法支持了补码有符号乘法器。为了能够利用计算机自动化求解，本文将寻找较优压缩操作的问题建模成数学问题，该问题的目标函数同时考虑了乘法器的精度和硬件开销，之后利用混合整数遗传算法进行搜索。
    本文对所提出的方法进行了实验，实验结果表明基于均匀分布得到的8比特无符号乘法器领先于国际前沿工作。针对基于8比特无符号数量化的三个不同规模的DNN生成的乘法器，在精度损失不超过0.01\%的情况下实现了26.4\%-47.6\%的硬件性能收益。此外，面向自适应滤波器生成的16位补码有符号乘法器在峰值信噪比（Peak Signal-to-Noise Ratio, PSNR）损失较小的情况下实现了27.1\%的硬件成本提升。最后，基于32比特半正态分布的实验结果表明提出的方法对大位宽乘法器同样有效。
    
    （2）由于ASIC和FPGA底层架构不同，ASIC近似乘法器通常无法在FPGA上取得相同比例的硬件性能提升，因此面向FPGA应用，本文基于贝叶斯优化提出了一个自动化近似乘法器生成方法，避免了手工修改查找表编码方法效率较低的问题。
    该方法假设乘法器的部分积在生成后、累加前存在一次由半加器阵列进行的压缩操作（与（1）中的压缩操作不同，这里是半加操作）。本文利用贝叶斯优化，基于所提出的4种半加器简化方法对半加器阵列进行优化。优化后，该方法保留后续累加过程中部分积的粗粒度加法，使其能够被电子设计自动化（Electronic Design Automation, EDA）工具高效地识别并映射到FPGA的快速进位链。与国际前沿工作中的1167个近似乘法器相比，本文生成的乘法器位于帕累拖前沿，精度和硬件开销综合指标平均提高了28.70\%-38.47\%。
    
    （3）结合前面两个工作，面向大规模电路，本文基于强化学习方法进行了近似逻辑综合研究。具体来讲，本文首先提出了一个基于最大无扇出锥（Maximum Fanout-Free Cone, MFFC）自适应超图划分的端到端强化学习逻辑优化框架，对大规模电路进行全面优化，以改善芯片的面积、延迟和功耗。
    % 该框架首先利用硬件描述语言解析工具Yosys对电路进行读入和解析，接着提取电路中组合逻辑，利用“自然划分”和MFFC超图划分将提取的组合逻辑分割成多个子电路，并利用强化学习序列优化方法对所有的子电路并行探索，最后由商业综合工具评估面积和延迟结果。
    本文对超过150个电路进行实验，实验结果表明所提出的方法与ABC resyn2 相比，面积延迟积平均提高了5.17\%。之后，本文将提出的强化学习逻辑优化框架与基于数据分布和输入极性得到的DNN近似乘法器库结合，对不同近似乘法器实现的DNN硬件加速器进行了实验，实验结果显示近似乘法器的单独硬件开销与对应加速器的硬件开销的改善比例存在一定偏差。然而，基于帕累拖前沿的乘法器实现的加速器仍处于帕累拖前沿。

    以上三个研究工作表明，面向大规模电路，硬件设计师在使用本文或其他的方法得到近似乘法器后，应对得到的帕累拖前沿乘法器进行探索以确定最佳的硬件实现。
    
\end{abstract}
    
\begin{abstract*}
    The rapid development of artificial intelligence requires massive computational power support, leading to a sharp increase in computing demands and consuming a significant amount of energy. At the same time, power consumption issues of integrated circuits in scenarios such as wearable devices, portable devices, and data centers are becoming increasingly severe. Researchers need to explore new methods to reduce system power consumption and improve chip efficiency. Approximate computing, as an emerging computing paradigm, allows systems to return results within an acceptable range of error. When combined with fault tolerance applications, it can improve computational efficiency and reduce chip power consumption while meeting accuracy requirements.
    
    Approximate circuit design is a branch of approximate computing aimed at reducing hardware overhead by introducing approximation to precise arithmetic units in circuits. Multiplication is a common computational operation frequently used in many applications. To enhance computational efficiency, researchers have proposed approximate multipliers to optimize multiplication operations in hardware. However, existing works on approximate multipliers neither simultaneously consider data distribution and input polarity nor rely on efficient manual design methods. Therefore, this thesis introduces two automated methods for designing approximate multipliers, capable of efficiently generating high-quality approximate multipliers of varying precision suitable for Application-Specific Integrated Circuits (ASICs) and Field Programmable Gate Arrays (FPGAs). Leveraging the obtained approximate multipliers, this thesis conducts research on approximate logic synthesis for hardware accelerators of Deep Neural Networks (DNNs) using reinforcement learning methods. The specific details are as follows:
    
    (1) Targeting ASICs, this thesis introduces an automated method for designing approximate multipliers based on data distribution and input polarity. This method efficiently generates high-performance ASIC approximate multipliers tailored to specific applications, thereby enhancing computational efficiency. Specifically, the method proposed in this thesis introduces AND, OR, XOR, and shift operations to compress partial products before partial product accumulation and summing, reducing the size of the partial product array and alleviating accumulation pressure. The thesis utilizes an enhanced Baugh-Wooley algorithm to support two's complement signed multipliers. To enable computer-aided optimization, the thesis models the problem of finding optimal compression operations as a mathematical problem. The objective function of this problem simultaneously considers the precision of the multiplier and hardware overhead, followed by a search using a mixed-integer genetic algorithm. Experimental validation of the proposed method shows that the 8-bit unsigned multipliers obtained from a uniform distribution outperform international state-of-the-art works. For multipliers generated for three different scales of DNNs based on 8-bit unsigned quantization, hardware performance gains of 26.4\% to 47.6\% were achieved with a precision loss not exceeding 0.01\%. Additionally, the 16-bit two's complement signed multipliers generated for adaptive filters achieved a 27.1\% hardware cost improvement with minimal Peak Signal-to-Noise Ratio (PSNR) loss. Finally, experimental results based on a 32-bit half-normal distribution demonstrate the effectiveness of the proposed method for large-bit-width multipliers.
    
    (2) Due to the different underlying architectures of ASICs and FPGAs, ASIC approximate multipliers typically cannot achieve the same level of hardware performance improvement on FPGAs. Therefore, tailored for FPGA applications, this thesis proposes an automated method for generating approximate multipliers based on Bayesian optimization, avoiding the inefficiencies of manually modifying lookup table encoding methods. This method assumes a compression operation by a carry-save adder array after the generation of partial products and before accumulation (different from the compression operation in (1), which involves half-adder operations). The thesis utilizes Bayesian optimization to optimize the carry-save adder array based on the four proposed half-adder simplification methods. After optimization, this method retains the coarse-grained addition of partial products during subsequent accumulation, enabling them to be efficiently identified and mapped to FPGA fast carry chains by Electronic Design Automation (EDA) tools. Compared to 1167 approximate multipliers from international state-of-the-art works, the multipliers generated in this thesis are at the Pareto frontier, with an average improvement of 28.70\% to 38.47\% in the combined metrics of precision and hardware overhead.

    (3) Combining the previous two works, this thesis focuses on large-scale circuits and conducts research on approximate logic synthesis using reinforcement learning methods. Specifically, the thesis first introduces an end-to-end reinforcement learning logic optimization framework based on Maximum Fanout-Free Cone (MFFC) adaptive hypergraph partitioning to comprehensively optimize large-scale circuits, aiming to improve chip area, delay, and power consumption. Experimental validation was performed on over 150 circuits, showing that the proposed method achieved an average improvement of 5.17\% in area-delay product compared to ABC resyn2.
    Subsequently, the thesis integrates the proposed reinforcement learning logic optimization framework with a DNN approximate multiplier library obtained based on data distribution and input polarity. Experiments were conducted on hardware accelerators for DNNs implemented with different approximate multipliers. The results indicate a certain deviation between the hardware overhead improvement of the individual approximate multipliers and the corresponding accelerator's hardware overhead. Nevertheless, accelerators implemented based on Pareto-optimal multipliers still remain at the Pareto frontier.
    
    The three research works above demonstrate that for large-scale circuits, hardware designers, after obtaining approximate multipliers using this thesis or other methods, should explore the Pareto-optimal multipliers obtained to determine the best hardware implementation.
    
\end{abstract*}